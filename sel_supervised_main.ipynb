{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import pickle as pkl\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from src.Transformer import retrieve_tf_names, GeneMasker, GRNInferModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argument setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = argparse.Namespace(\n",
    "    alpha = 0.01, # scale hyperparameter for the loss regularisation term\n",
    "    attention_dim = 64, # dimensionality of the attention layer\n",
    "    attention_heads = 4, # number of attention heads\n",
    "    batchsize = 32,\n",
    "    dataset = \"\",\n",
    "    data_file = \"./demo_data/BEELINE/500_ChIP-seq_mDC/inputs/data.csv\", # path of input scRNA-seq file\n",
    "    dropout = 0.0, # Dropout probability in the dropout layers of the model\n",
    "    dorothea_grade = \"A\", # the minimum allowable Dorothea letter grade for transcription factors\n",
    "    embed_dim = 64, # embedding dimension\n",
    "    ffn_embed_dim = 64, # embedding dimension for both feed forward networks of the model\n",
    "    lr = 1e-4, # learning rate\n",
    "    n_epochs = 1500, # number of epochs of training\n",
    "    number_report_score = 4, # the minimum allowable Number Report score for transcription factors\n",
    "    PIDC_file = \"./demo_data/BEELINE/500_ChIP-seq_mDC/inputs/PIDC_output_file.txt\",\n",
    "    save_name = 'pretrain_output',\n",
    "    seed = 0,\n",
    "    tf_data_file = \"transcription_factors.csv\", # path of transcription factor csv file\n",
    "    top_k = 20, # The number of largest attention weights to retain when calculating loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available(): # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(opt.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data preprocessing\n"
     ]
    }
   ],
   "source": [
    "def init_data(opt):\n",
    "\n",
    "    # load data\n",
    "    data = pd.read_csv(opt.data_file, header = 0, index_col = 0).T\n",
    "    data_values = data.values\n",
    "\n",
    "    ###########################################################################\n",
    "    # this section is not required once we have a proper embedding to work with\n",
    "\n",
    "    # build masks for data (nonzero values are true)\n",
    "    d_mask_np = (data_values != 0).astype(float)\n",
    "    d_mask = torch.FloatTensor(d_mask_np)\n",
    "\n",
    "    # get mean and std for each column (excluding zero entries)\n",
    "    means = []\n",
    "    stds = []\n",
    "    # loop through each column\n",
    "    for i in range(data_values.shape[1]):\n",
    "        tmp = data_values[:, i]\n",
    "\n",
    "        # is there any non-zero entries in this column\n",
    "        if sum(tmp != 0) == 0:\n",
    "            # every entry is zero\n",
    "            means.append(0)\n",
    "            stds.append(1)\n",
    "            \n",
    "        else:\n",
    "            # append mean and std of nonzero entries\n",
    "            means.append(tmp[tmp != 0].mean())\n",
    "            stds.append(tmp[tmp != 0].std())\n",
    "\n",
    "    # convert to np.array\n",
    "    means = np.array(means)\n",
    "    stds = np.array(stds)\n",
    "\n",
    "    # set any Nan or inf entries of means to 1\n",
    "    means[np.isnan(stds)] = 0\n",
    "    means[np.isinf(stds)] = 0\n",
    "\n",
    "    # set any Nan, inf, or zero entries of stds to 1\n",
    "    stds[(np.isnan(stds))] = 1\n",
    "    stds[np.isinf(stds)] = 1\n",
    "    stds[stds == 0] = 1\n",
    "    \n",
    "    # shift each column by it's mean and scale it by it's std\n",
    "    data_values = (data_values - means) / (stds)\n",
    "\n",
    "    # Set any nan or inf entries to 0\n",
    "    data_values[np.isnan(data_values)] = 0\n",
    "    data_values[np.isinf(data_values)] = 0\n",
    "\n",
    "    # Set any entries less then -20 to -20\n",
    "    data_values = np.maximum(data_values, -20)\n",
    "    # Set any entries greater then 20 to 20\n",
    "    data_values = np.minimum(data_values, 20)\n",
    "\n",
    "    ###########################################################################\n",
    "\n",
    "    # store as dataframe and float tensor\n",
    "    data = pd.DataFrame(data_values, index = data.index, columns = data.columns)\n",
    "    feat_train = torch.FloatTensor(data.values)\n",
    "\n",
    "    return feat_train, d_mask, data.columns\n",
    "\n",
    "\n",
    "input_all, d_mask, gene_name = init_data(opt)\n",
    "tf_data = pd.read_csv(opt.tf_data_file)\n",
    "tf_names = retrieve_tf_names(\n",
    "    gene_name, \n",
    "    tf_data, \n",
    "    opt.dorothea_grade, \n",
    "    opt.number_report_score\n",
    ")\n",
    "print('Finished data preprocessing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch if we have an embedding of dimension 1\n",
    "# we want a shape of [# cells, # genes, embedding dim]\n",
    "if input_all.dim() == 2:\n",
    "    # the embedding for each gene is 1\n",
    "    input_all = input_all.unsqueeze(-1)\n",
    "    model_input_dim = 1\n",
    "    \n",
    "else:\n",
    "    model_input_dim = input_all.shape[-1]\n",
    "\n",
    "# set up model\n",
    "model = GRNInferModel(\n",
    "    input_dim = model_input_dim, \n",
    "    attn_dim = opt.attention_dim,\n",
    "    num_heads = opt.attention_heads, \n",
    "    pre_ffn_embed_dim = opt.ffn_embed_dim,\n",
    "    post_ffn_embed_dim = opt.ffn_embed_dim,\n",
    "    alpha = opt.alpha,\n",
    "    top_k = opt.top_k,\n",
    "    dropout = opt.dropout, \n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masker setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup masking object              \n",
    "g_masker = GeneMasker(\n",
    "    genes = gene_name,\n",
    "    t_factors = tf_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimiser\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr = opt.lr, \n",
    "    betas=(0.9, 0.999), \n",
    "    weight_decay = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup data loader\n",
    "dataset = TensorDataset(\n",
    "    input_all, \n",
    "    d_mask, \n",
    "    torch.LongTensor(list(range(len(input_all))))\n",
    ")\n",
    "\n",
    "drop_last = True\n",
    "if len(input_all) < opt.batchsize:\n",
    "    drop_last = False\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size = opt.batchsize, \n",
    "    shuffle = True, \n",
    "    num_workers = 1, \n",
    "    drop_last = drop_last\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished training epoch: 2; loss: 1.0868526697158813:   0%|          | 3/1500 [00:47<6:31:21, 15.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1338780/2894336344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# backward step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "loss_save = []\n",
    "t = tqdm(range(opt.n_epochs))\n",
    "for epoch in t: #tqdm(range(opt.n_epochs)):\n",
    "    loss_all = []\n",
    "    model = model.to(device)\n",
    "\n",
    "    for X, mask, idn in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X_clone = X.clone()\n",
    "        X = X.to(device)\n",
    "\n",
    "        # mask non-transcription factor genes in input X\n",
    "        g_masker.mask_non_tfs(X)\n",
    "                                \n",
    "        # forward pass\n",
    "        output, attn = model(X, return_attn = True)\n",
    "\n",
    "        # calculate loss          \n",
    "        # mask the transcription factors for the original input to the model and the models output\n",
    "        g_masker.mask_tfs(output)\n",
    "        g_masker.mask_tfs(X_clone)\n",
    "\n",
    "        loss = model.loss(\n",
    "            output, \n",
    "            X_clone.to(device), \n",
    "            attn \n",
    "        )\n",
    "\n",
    "        # backward step\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), 0.001)\n",
    "        optimizer.step()\n",
    "\n",
    "        # record loss\n",
    "        loss_all.append(loss)\n",
    "\n",
    "    # update tqdm ticker\n",
    "    t.set_description(f'Finished training epoch: {epoch}; loss: {torch.stack(loss_all).mean()}')\n",
    "\n",
    "    # \n",
    "    loss_save.append(torch.stack(loss_all).mean().cpu().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin generate candidates GRN features\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1338780/2071663705.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Maths/4th Year/2023 Autumn/CMPT415/GRN-infer/src/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_attn)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Attention part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mattn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Maths/4th Year/2023 Autumn/CMPT415/GRN-infer/src/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_attention)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mQKV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQKV_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "print('Begin generate candidates GRN features')\n",
    "test_device = device \n",
    "\n",
    "model = model.to(test_device)\n",
    "input = input_all.clone().to(test_device).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attn = model(input, return_attn = True)\n",
    "\n",
    "adj = []\n",
    "for i in range(attn.shape[-1]): \n",
    "    adj.append(attn[:, :, i].cup().numpy())\n",
    "\n",
    "adj = np.array(adj)\n",
    "pkl.dump([adj, loss_save], open(f'{opt.save_name}', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
